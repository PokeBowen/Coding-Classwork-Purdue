knitr::opts_chunk$set(echo = TRUE)
qb_model <- glm(activity ~ ., family = quasibinomial, data = pyrimidines_clean)
knitr::opts_chunk$set(echo = TRUE)
data("esoph")
full_model <- glm(cbind(ncases, ncontrols) ~ (agegp + alcgp + tobgp)^2, family = binomial, data = esoph)
best_model <- step(full_model, direction = "both")
summary(best_model)
# Create the new predictors
esoph$age <- unclass(esoph$agegp) - 3.5
esoph$alc <- unclass(esoph$alcgp) - 2.5
esoph$tob <- unclass(esoph$tobgp) - 2.5
# Our simplified model from our previous part a results
simplified_model <- glm(cbind(ncases, ncontrols) ~ age + I(age^2) + alc + tob, family = binomial, data = esoph)
summary(simplified_model)
pchisq(93.172, 83, lower=FALSE)
summary(simplified_model)$deviance / summary(simplified_model)$df.residual
library(statmod)
q_resids <- qres.binom(simplified_model)
plot(fitted(simplified_model), q_resids, xlab = "Fitted Probabilities", ylab = "Quantile Residuals", main = "Random Quantile Residual Plot")
abline(h = 0, lty = 2, col = "blue")
par(mfrow = c(2, 2))
plot(simplified_model)
tob_coef <- summary(simplified_model)$coefficients["tob", "Estimate"]
tob_se <- summary(simplified_model)$coefficients["tob", "Std. Error"]
lower_log <- tob_coef - 1.96 * tob_se
upper_log <- tob_coef + 1.96 * tob_se
# We take exp to get to odds ratio
exp(tob_coef)
OR_lower <- exp(lower_log)
OR_upper <- exp(upper_log)
cat(OR_lower, OR_upper)
# Or use built in R function that I found out only later...
exp(confint(simplified_model, "tob"))
beta_hat <- coef(simplified_model)
V <- vcov(simplified_model)
# Our contrast vector 'L' for the change from age group 3 to 4:
# Change in age: (0.5) - (-0.5) = 1
# Change in age^2: (0.25) - (0.25) = 0
# The vector matches the order of coefficients in our model: (Intercept, age, I(age^2), alc, tob)
L <- c(0, 1, 0, 0, 0)
# Estimate of the effect (Log-Odds)
est_effect <- t(L) %*% beta_hat
# Standard Error of the effect
# SE = sqrt( L' * V * L )
se_effect <- sqrt(t(L) %*% V %*% L)
lower_log <- est_effect - 1.96 * se_effect
upper_log <- est_effect + 1.96 * se_effect
exp(c(lower_log, upper_log))
# Or use R function to find it easily
exp(confint(simplified_model, "age"))
library(faraway)
data(pyrimidines)
# Plotting activity against the first three predictors
par(mfrow=c(1,3))
plot(activity ~ p1.polar, data=pyrimidines, main="Activity vs p1.polar")
plot(activity ~ p1.size, data=pyrimidines, main="Activity vs p1.size")
plot(activity ~ p1.flex, data=pyrimidines, main="Activity vs p1.flex")
outlier_index <- which(pyrimidines$activity < 0.2)
print(outlier_index)
pyrimidines_clean <- pyrimidines[-outlier_index, ]
linear_model <- lm(activity ~ ., data = pyrimidines_clean)
summary(linear_model)
par(mfrow = c(2, 2))
plot(linear_model)
qb_model <- glm(activity ~ ., family = quasibinomial, data = pyrimidines_clean)
summary(qb_model)
# Plot
preds_gaussian <- predict(linear_model)
preds_quasi <- predict(qb_model, type = "response")
plot(preds_gaussian, preds_quasi,
xlab = "Gaussian Predictions",
ylab = "Quasi-binomial Predictions",
main = "Comparison of Predicted Values")
abline(0, 1, col = "red") # 45-degree line for perfect agreement
coef_gaussian_logit <- coef(linear_model)
coef_quasibinomial <- coef(qb_model)
data.frame(
linear_model = coef_gaussian_logit,
Quasi_Binomial = coef_quasibinomial,
Difference = coef_gaussian_logit - coef_quasibinomial
)
pyrimidines_clean$logit_activity <- log(pyrimidines_clean$activity / (1 - pyrimidines_clean$activity))
lmod_logit <- lm(logit_activity ~ . - activity, data = pyrimidines_clean)
# Plot
inv_logit <- function(x) exp(x) / (1 + exp(x))
preds_gaussian <- inv_logit(predict(lmod_logit))
preds_quasi <- predict(qb_model, type = "response")
plot(preds_gaussian, preds_quasi,
xlab = "Logit Gaussian Predictions",
ylab = "Quasi-binomial Predictions",
main = "Comparison of Predicted Values")
abline(0, 1, col = "red") # 45-degree line for perfect agreement
coef_gaussian_logit <- coef(lmod_logit)
coef_quasibinomial <- coef(qb_model)
data.frame(
Gaussian_Logit = coef_gaussian_logit,
Quasi_Binomial = coef_quasibinomial,
Difference = coef_gaussian_logit - coef_quasibinomial
)
# Most of the code is kind of guesswork bc Prof Song didn't provide example code for this package
library(mgcv)
# Get all predictor names (excluding activity and any temporary logit columns)
predictors <- names(pyrimidines)[!names(pyrimidines) %in% c("activity", "logit_activity")]
# Construct the formula: "activity ~ p1 + p2 + ... + p26"
gam_formula <- as.formula(paste("activity ~", paste(predictors, collapse = " + ")))
bgam <- gam(gam_formula,  family = betar(link="logit"), data = pyrimidines_clean)
beta_gauss <- coef(lmod_logit)[-1]
beta_gam   <- coef(bgam)[-1]
data.frame(
Predictor = names(beta_gauss),
Logit_Gaussian = round(beta_gauss, 4),
Beta_GAM = round(beta_gam, 4),
Difference = round(beta_gauss - beta_gam, 4)
)
library(ggplot2)
library(tidyr)
library(dplyr)
set.seed(42)
N_sims <- 1000
settings <- expand.grid(m = c(5, 15, 45), n = c(50, 200, 800))
results_list <- list()
# Loop through our 9 settings
for (i in 1:nrow(settings)) {
m_val <- settings$m[i]
n_val <- settings$n[i]
deviance_vals <- numeric(N_sims)
pearson_vals <- numeric(N_sims)
for (j in 1:N_sims) {
# 1. Generate Data
x <- rnorm(n_val, 0, 1)
logit_p <- 0.35 + x
p <- 1 / (1 + exp(-logit_p))
y <- rbinom(n_val, size = m_val, prob = p)
# 2. Fit Model
model <- glm(cbind(y, m_val - y) ~ x, family = binomial)
# 3. Save Measures
deviance_vals[j] <- deviance(model)
pearson_vals[j] <- sum(residuals(model, type = "pearson")^2)
}
# Store results in a data frame
results_list[[i]] <- data.frame(
setting = paste0("m=", m_val, ", n=", n_val),
m = m_val,
n = n_val,
Deviance = deviance_vals,
Pearson = pearson_vals
)
}
# Combine all results
all_results <- bind_rows(results_list)
# Reshape for plotting
all_data <- bind_rows(all_results) %>%
pivot_longer(cols = c(Deviance, Pearson), names_to = "Measure", values_to = "Stat")
df_50 <- 50 - 2
ggplot(filter(all_data, n == 50), aes(x = Stat)) +
geom_histogram(aes(y = ..density..), bins = 35, fill = "skyblue", color = "white") +
stat_function(fun = dchisq, args = list(df = df_50), color = "red", size = 1) +
facet_grid(Measure ~ m, labeller = label_both) +
labs(title = "Settings 1-3: n = 50 cases", subtitle = "Red line: Chi-square (df = 48)") +
theme_minimal()
df_200 <- 200 - 2
ggplot(filter(all_data, n == 200), aes(x = Stat)) +
geom_histogram(aes(y = ..density..), bins = 35, fill = "steelblue", color = "white") +
stat_function(fun = dchisq, args = list(df = df_200), color = "red", size = 1) +
facet_grid(Measure ~ m, labeller = label_both) +
labs(title = "Settings 4-6: n = 200 cases", subtitle = "Red line: Chi-square (df = 198)") +
theme_minimal()
df_800 <- 800 - 2
ggplot(filter(all_data, n == 800), aes(x = Stat)) +
geom_histogram(aes(y = ..density..), bins = 35, fill = "midnightblue", color = "white") +
stat_function(fun = dchisq, args = list(df = df_800), color = "red", size = 1) +
facet_grid(Measure ~ m, labeller = label_both) +
labs(title = "Settings 7-9: n = 800 cases", subtitle = "Red line: Chi-square (df = 798)") +
theme_minimal()
